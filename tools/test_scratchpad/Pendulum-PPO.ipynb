{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksuran/Atari_DQN/blob/master/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "udtLxTQyiNUz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "# sh -c 'pip install tensorflow tensorflow-metal tqdm gymnasium[classic-control] tensorflow_probability ipywidgets matplotlib pyvirtualdisplay'\n",
        "import sys\n",
        "sys.path.append('../Simulator')\n",
        "\n",
        "import motoflex_gym\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/marload/DeepRL-TensorFlow2/blob/master/PPO/PPO_Continuous.py\n",
        "\n",
        "From example above, however, it does not train at the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/civic/dev/MoToFlex/.venv/lib/python3.11/site-packages/gymnasium/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in (old) done step API which returns one bool instead of two.\u001b[0m\n",
            "  deprecation(\n",
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        self.std_bound = std_bound\n",
        "        self.model = self.create_model()\n",
        "        self.opt = tf.keras.optimizers.Adam(args.actor_lr)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = np.reshape(state, [1, self.state_dim])\n",
        "        mu, std = self.model.predict(state, verbose=0)\n",
        "        action = np.random.normal(mu[0], std[0], size=self.action_dim)\n",
        "        action = np.clip(action, -self.action_bound, self.action_bound)\n",
        "        log_policy = self.log_pdf(mu, std, action)\n",
        "\n",
        "        return log_policy, action\n",
        "\n",
        "    def log_pdf(self, mu, std, action):\n",
        "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
        "        var = std ** 2\n",
        "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
        "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
        "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
        "\n",
        "    def create_model(self):\n",
        "        state_input = Input((self.state_dim,))\n",
        "        dense_1 = Dense(32, activation='relu')(state_input)\n",
        "        dense_2 = Dense(32, activation='relu')(dense_1)\n",
        "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
        "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
        "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
        "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
        "\n",
        "    def compute_loss(self, log_old_policy, log_new_policy, actions, gaes):\n",
        "        ratio = tf.exp(log_new_policy - tf.stop_gradient(log_old_policy))\n",
        "        gaes = tf.stop_gradient(gaes)\n",
        "        clipped_ratio = tf.clip_by_value(\n",
        "            ratio, 1.0-args.clip_ratio, 1.0+args.clip_ratio)\n",
        "        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n",
        "        return tf.reduce_mean(surrogate)\n",
        "\n",
        "    def train(self, log_old_policy, states, actions, gaes):\n",
        "        with tf.GradientTape() as tape:\n",
        "            mu, std = self.model(states, training=True)\n",
        "            log_new_policy = self.log_pdf(mu, std, actions)\n",
        "            loss = self.compute_loss(\n",
        "                log_old_policy, log_new_policy, actions, gaes)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, state_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.model = self.create_model()\n",
        "        self.opt = tf.keras.optimizers.Adam(args.critic_lr)\n",
        "\n",
        "    def create_model(self):\n",
        "        return tf.keras.Sequential([\n",
        "            Input((self.state_dim,)),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "    def compute_loss(self, v_pred, td_targets):\n",
        "        mse = tf.keras.losses.MeanSquaredError()\n",
        "        return mse(td_targets, v_pred)\n",
        "\n",
        "    def train(self, states, td_targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            v_pred = self.model(states, training=True)\n",
        "            assert v_pred.shape == td_targets.shape\n",
        "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "    \n",
        "\n",
        "class Args:\n",
        "    gamma = 0.9\n",
        "    update_interval = 1024\n",
        "    actor_lr = 0.001\n",
        "    critic_lr = 0.001\n",
        "    clip_ratio = 0.2\n",
        "    lmbda = 0.95\n",
        "    epochs = 10\n",
        "args = Args()\n",
        "\n",
        "\n",
        "def gae_target(rewards, v_values, next_v_value, done):\n",
        "    n_step_targets = np.zeros_like(rewards)\n",
        "    gae = np.zeros_like(rewards)\n",
        "    gae_cumulative = 0\n",
        "    forward_val = 0\n",
        "\n",
        "    if not done:\n",
        "        forward_val = next_v_value\n",
        "\n",
        "    for k in reversed(range(0, len(rewards))):\n",
        "        delta = rewards[k] + args.gamma * forward_val - v_values[k]\n",
        "        gae_cumulative = args.gamma * args.lmbda * gae_cumulative + delta\n",
        "        gae[k] = gae_cumulative\n",
        "        forward_val = v_values[k]\n",
        "        n_step_targets[k] = gae[k] + v_values[k]\n",
        "    return gae, n_step_targets\n",
        "\n",
        "def list_to_batch(list):\n",
        "    batch = list[0]\n",
        "    for elem in list[1:]:\n",
        "        batch = np.append(batch, elem, axis=0)\n",
        "    return batch\n",
        "\n",
        "env_name = 'Pendulum-v1'\n",
        "env = gym.wrappers.StepAPICompatibility(gym.make(env_name), output_truncation_bool=False)\n",
        "\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_bound = env.action_space.high[0]\n",
        "std_bound = [1e-2, 1.0]\n",
        "\n",
        "actor_opt = tf.keras.optimizers.Adam(args.actor_lr)\n",
        "critic_opt = tf.keras.optimizers.Adam(args.critic_lr)\n",
        "actor = Actor(state_dim, action_dim, action_bound, std_bound)\n",
        "critic = Critic(state_dim)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/civic/dev/MoToFlex/.venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/pendulum.py:173: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP0 EpisodeReward=-1562.6533129608545\n",
            "EP1 EpisodeReward=-1139.509778651463\n",
            "EP2 EpisodeReward=-1566.8020709618925\n",
            "EP3 EpisodeReward=-1168.0199239634228\n",
            "EP4 EpisodeReward=-1618.6212655111715\n",
            "EP5 EpisodeReward=-1049.8182734672828\n",
            "EP6 EpisodeReward=-1463.216106662559\n",
            "EP7 EpisodeReward=-1516.6114004834353\n",
            "EP8 EpisodeReward=-1584.7969123308096\n",
            "EP9 EpisodeReward=-1378.7188170646084\n",
            "EP10 EpisodeReward=-1473.9960963525882\n",
            "EP11 EpisodeReward=-1592.4272593928076\n",
            "EP12 EpisodeReward=-1557.7795137251974\n",
            "EP13 EpisodeReward=-1176.4335606145364\n",
            "EP14 EpisodeReward=-1608.9070058911414\n",
            "EP15 EpisodeReward=-1290.0069241038827\n",
            "EP16 EpisodeReward=-1715.7167896754243\n",
            "EP17 EpisodeReward=-1566.3592117113672\n",
            "EP18 EpisodeReward=-1675.1771293390054\n",
            "EP19 EpisodeReward=-1534.751594399259\n",
            "EP20 EpisodeReward=-1253.2697817699902\n",
            "EP21 EpisodeReward=-1169.335416307532\n",
            "EP22 EpisodeReward=-1195.8207226330783\n",
            "EP23 EpisodeReward=-933.5179947493388\n",
            "EP24 EpisodeReward=-1458.8507598800263\n",
            "EP25 EpisodeReward=-1431.4488715250545\n",
            "EP26 EpisodeReward=-1557.6456063872893\n",
            "EP27 EpisodeReward=-1083.4075472049356\n",
            "EP28 EpisodeReward=-1341.8132982411637\n",
            "EP29 EpisodeReward=-1172.8384773827845\n",
            "EP30 EpisodeReward=-1287.3831407257112\n",
            "EP31 EpisodeReward=-1599.5083585148957\n",
            "EP32 EpisodeReward=-1575.0789677632667\n",
            "EP33 EpisodeReward=-1071.7412948324936\n",
            "EP34 EpisodeReward=-1460.485769825437\n",
            "EP35 EpisodeReward=-1065.0636277192725\n",
            "EP36 EpisodeReward=-1218.6415838737296\n",
            "EP37 EpisodeReward=-1583.2809835105406\n",
            "EP38 EpisodeReward=-1282.0641134978987\n",
            "EP39 EpisodeReward=-1714.302084679638\n",
            "EP40 EpisodeReward=-1562.460275090239\n",
            "EP41 EpisodeReward=-1098.5772526292617\n",
            "EP42 EpisodeReward=-1188.4277878591906\n",
            "EP43 EpisodeReward=-1609.696856896249\n",
            "EP44 EpisodeReward=-1581.3646607793303\n",
            "EP45 EpisodeReward=-1334.6863709244237\n",
            "EP46 EpisodeReward=-1352.9527499163653\n",
            "EP47 EpisodeReward=-1280.8561216340775\n",
            "EP48 EpisodeReward=-1592.280016685796\n",
            "EP49 EpisodeReward=-838.183280804451\n",
            "EP50 EpisodeReward=-1054.2982047980195\n",
            "EP51 EpisodeReward=-1247.707795221473\n",
            "EP52 EpisodeReward=-1608.2184969279992\n",
            "EP53 EpisodeReward=-1544.1965592841184\n",
            "EP54 EpisodeReward=-1730.1467728543023\n",
            "EP55 EpisodeReward=-1402.3324903373111\n",
            "EP56 EpisodeReward=-1634.7952031085601\n",
            "EP57 EpisodeReward=-1251.4332341298175\n",
            "EP58 EpisodeReward=-1378.428224802508\n",
            "EP59 EpisodeReward=-1592.6312090876472\n",
            "EP60 EpisodeReward=-1542.9242364100992\n",
            "EP61 EpisodeReward=-1532.2891580116204\n",
            "EP62 EpisodeReward=-1613.8856377235043\n",
            "EP63 EpisodeReward=-1613.9747307912687\n",
            "EP64 EpisodeReward=-1414.601428995158\n",
            "EP65 EpisodeReward=-1608.436763641014\n",
            "EP66 EpisodeReward=-1411.3019581920896\n",
            "EP67 EpisodeReward=-1487.113397043946\n",
            "EP68 EpisodeReward=-1396.938866155025\n",
            "EP69 EpisodeReward=-1435.1057832975482\n",
            "EP70 EpisodeReward=-1345.1089368060407\n",
            "EP71 EpisodeReward=-1327.063190756544\n",
            "EP72 EpisodeReward=-1578.736373343222\n",
            "EP73 EpisodeReward=-1387.3277855379126\n",
            "EP74 EpisodeReward=-994.584171211212\n",
            "EP75 EpisodeReward=-1549.8236416709963\n",
            "EP76 EpisodeReward=-1477.3642832558808\n",
            "EP77 EpisodeReward=-1568.9425186397168\n",
            "EP78 EpisodeReward=-1588.69764552921\n",
            "EP79 EpisodeReward=-1559.6303073821766\n",
            "EP80 EpisodeReward=-1542.7248885025208\n",
            "EP81 EpisodeReward=-1087.1066650825537\n",
            "EP82 EpisodeReward=-1549.6088409285276\n",
            "EP83 EpisodeReward=-1234.6906413035904\n",
            "EP84 EpisodeReward=-1538.4976485291272\n",
            "EP85 EpisodeReward=-1550.8034167931487\n",
            "EP86 EpisodeReward=-1402.723423185476\n",
            "EP87 EpisodeReward=-1371.7783899408946\n",
            "EP88 EpisodeReward=-1407.354003282456\n",
            "EP89 EpisodeReward=-1209.9887053406878\n",
            "EP90 EpisodeReward=-1549.7028617459093\n",
            "EP91 EpisodeReward=-1431.2936347042978\n",
            "EP92 EpisodeReward=-1489.1654432242854\n",
            "EP93 EpisodeReward=-1304.3254834815807\n",
            "EP94 EpisodeReward=-1451.9743563017666\n",
            "EP95 EpisodeReward=-1191.7063213561958\n",
            "EP96 EpisodeReward=-1438.783173780019\n",
            "EP97 EpisodeReward=-1545.3055195776553\n",
            "EP98 EpisodeReward=-1427.95403519918\n",
            "EP99 EpisodeReward=-1469.3880723439881\n",
            "EP100 EpisodeReward=-1328.9557227357193\n",
            "EP101 EpisodeReward=-1297.9016784806126\n",
            "EP102 EpisodeReward=-1528.6230962207112\n",
            "EP103 EpisodeReward=-1267.7377861182845\n",
            "EP104 EpisodeReward=-1396.8054109047594\n",
            "EP105 EpisodeReward=-1530.1726148768314\n",
            "EP106 EpisodeReward=-1233.3897294576216\n",
            "EP107 EpisodeReward=-1409.5182629921158\n",
            "EP108 EpisodeReward=-1440.5710306205988\n",
            "EP109 EpisodeReward=-1220.0734861088029\n",
            "EP110 EpisodeReward=-1208.3866449795898\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     13\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 14\u001b[0m     log_old_policy, action \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     18\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, state_dim])\n",
            "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mActor.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     13\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim])\n\u001b[0;32m---> 14\u001b[0m     mu, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(mu[\u001b[38;5;241m0\u001b[39m], std[\u001b[38;5;241m0\u001b[39m], size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim)\n\u001b[1;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_bound, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_bound)\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:2651\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_tuner\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   2650\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menumerate_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single epoch.\u001b[39;49;00m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatch_stop_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1341\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset)\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:500\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    499\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:706\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    702\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    705\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 706\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:745\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    743\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    744\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 745\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/MoToFlex/.venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3421\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3420\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3421\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3422\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3424\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "max_episodes = 10000\n",
        "for ep in range(max_episodes):\n",
        "    state_batch = []\n",
        "    action_batch = []\n",
        "    reward_batch = []\n",
        "    old_policy_batch = []\n",
        "\n",
        "    episode_reward, done = 0, False\n",
        "\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "        log_old_policy, action = actor.get_action(state)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        state = np.reshape(state, [1, state_dim])\n",
        "        action = np.reshape(action, [1, 1])\n",
        "        next_state = np.reshape(next_state, [1, state_dim])\n",
        "        reward = np.reshape(reward, [1, 1])\n",
        "        log_old_policy = np.reshape(log_old_policy, [1, 1])\n",
        "\n",
        "        state_batch.append(state)\n",
        "        action_batch.append(action)\n",
        "        reward_batch.append((reward+8)/8) # Why +8? Maybe sign has changed with new versions.\n",
        "        old_policy_batch.append(log_old_policy)\n",
        "\n",
        "        if len(state_batch) >= args.update_interval or done:\n",
        "            states = list_to_batch(state_batch)\n",
        "            actions = list_to_batch(action_batch)\n",
        "            rewards = list_to_batch(reward_batch)\n",
        "            old_policys = list_to_batch(old_policy_batch)\n",
        "\n",
        "            v_values = critic.model.predict(states, verbose=0)\n",
        "            next_v_value = critic.model.predict(next_state, verbose=0) # No batch like the others?\n",
        "\n",
        "            gaes, td_targets = gae_target(\n",
        "                rewards, v_values, next_v_value, done)\n",
        "\n",
        "            for epoch in range(args.epochs):\n",
        "                actor_loss = actor.train(\n",
        "                    old_policys, states, actions, gaes)\n",
        "                critic_loss = critic.train(states, td_targets)\n",
        "\n",
        "            state_batch = []\n",
        "            action_batch = []\n",
        "            reward_batch = []\n",
        "            old_policy_batch = []\n",
        "\n",
        "        episode_reward += reward[0][0]\n",
        "        state = next_state[0]\n",
        "\n",
        "    print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNqAoPgkLLw0YI6ipHp8Hzq",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "PPO.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
